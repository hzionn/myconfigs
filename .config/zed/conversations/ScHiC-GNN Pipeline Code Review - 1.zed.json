{"id":"34e22d5e-17a4-43dd-a40f-eeb5640b2612","zed":"context","version":"0.4.0","text":"\n\n\n```py schic-gnn/schicgnn/pipeline.py\n\"\"\"\nthis is the whole pipeline for clustering single-cell Hi-C data on one dataset.\n\"\"\"\n\nimport logging\nfrom pathlib import Path\nfrom time import time\nfrom typing import List, Optional, Tuple\n\nimport numpy as np\nimport torch\nfrom sklearn.cluster import KMeans\nfrom sklearn.decomposition import PCA\nfrom torch_geometric.data import Data\nfrom tqdm import tqdm\n\nfrom schicgnn.config import Config\nfrom schicgnn.data.data_loader import DataLoader, Dataset\nfrom schicgnn.models.gnn import HiCGNN, HiCGAT\nfrom schicgnn.preprocessing.enhancing import HiCMFR\nfrom schicgnn.preprocessing.transform import get_n_pc\nfrom schicgnn.utils.helpers import filter_cells\nfrom schicgnn.utils.metrics import ari_score, fm_score, hm_score, nmi_score\nfrom schicgnn.utils.visualize import (\n    plot_pca_clusters,\n    plot_umap_cell_types,\n    umap_reducer,\n    plot_evaluation_scores,\n)\n\nclass Pipeline:\n    \"\"\"\n    here we will call the functions in these steps:\n    1. setup_dataset\n    2. load_data\n    3. build_features\n    4. cluster\n    5. evaluate\n    6. visualize\n    \"\"\"\n\n    def __init__(self, config: Config):\n        self.config = config\n        self.dataset: Optional[Dataset] = None\n        self.cell_type_id_bins: Optional[dict] = None\n        self.features: Optional[np.ndarray] = None\n        self.cell_labels: Optional[List[Tuple[str, str]]] = None\n        self.clusters: Optional[np.ndarray] = None\n        self.features_reduced: Optional[np.ndarray] = None\n        self.metrics: Optional[dict] = None\n        self.timestamp = int(time())\n        logging.basicConfig(\n            level=logging.INFO,\n            format=\"%(asctime)s - %(levelname)s - %(message)s\",\n            datefmt=\"%H:%M:%S\",\n            handlers=[\n                logging.FileHandler(self.config.dataset_name + \".log\"),\n                logging.StreamHandler()\n            ],\n        )\n        logging.info(f\"{'-'*25}{self.timestamp}{'-'*25}\")\n        logging.info(f\"Initialized pipeline with config: {self.config}\")\n\n    def setup_dataset(self, dataset_path: Path, name: str, species: str):\n        self.dataset = Dataset(dataset_dir=dataset_path, name=name, species=species)\n        logging.info(f\"Dataset directory: {self.dataset.dataset_dir}\")\n        logging.info(f\"Dataset name: {self.dataset.name}\")\n        logging.info(f\"Species: {self.dataset.species}\")\n        logging.info(f\"Cell types: {self.dataset.cell_types}\")\n\n    def load_data(self, base_path: Path):\n        if self.dataset is None:\n            raise ValueError(\"Dataset is not set. Call setup_dataset first.\")\n        self.cell_type_id_bins = DataLoader(base_path, self.config.recache).load_dataset(self.dataset)\n\n    def build_features(self):\n        features, cell_labels = self._extract_features()\n        self.cell_labels = cell_labels\n        self.features = self._truncate_features(features)\n        self.features_reduced = self._reduce_features(self.features)\n\n    def _extract_features(self) -> Tuple:\n        if self.dataset is None:\n            raise ValueError(\"Dataset is not set. Call setup_dataset first.\")\n\n        # human has 23 chromosome, mouse has only 20\n        n_chromosomes = 23 if self.dataset.species == \"human\" else 20\n        features = []\n        cell_labels = []\n\n        if self.cell_type_id_bins is None:\n            raise ValueError(\"Cell ID bins are not loaded. Call load_data first.\")\n\n        total_remaining_cells = 0\n        for cell_type, cell_ids_bins in self.cell_type_id_bins.items():\n            count = len(self.cell_type_id_bins[cell_type])\n            logging.info(f\"Cell type: {cell_type}, Count: {count}\")\n            remaining_cells = filter_cells(n_components=self.config.pca_n_components, n_chromosome=n_chromosomes, dataset_name=self.dataset.name, cell_type=cell_type, cells=cell_ids_bins)\n            logging.info(f\"# cells after filtering({cell_type}): {len(remaining_cells)}\")\n            features_batch, labels_batch = self._process_cell_batch(\n                cell_type=cell_type,\n                cell_ids_bins=cell_ids_bins,\n                remaining_cells=remaining_cells,\n                n_chromosomes=n_chromosomes,\n            )\n            features.extend(features_batch)\n            cell_labels.extend(labels_batch)\n            total_remaining_cells += len(remaining_cells)\n\n        logging.info(f\"Total remaining cells all cell types: {total_remaining_cells}\")\n        return features, cell_labels\n\n    def _process_cell_batch(\n        self, cell_type: str, cell_ids_bins, remaining_cells: List[str], n_chromosomes: int\n    ) -> Tuple[List, List[Tuple]]:\n        features_batch = []\n        labels_batch = []\n        assert len(remaining_cells) != 0\n        for cell_id, cell_data in tqdm(cell_ids_bins.items(), ncols=100):\n            assert type(cell_id) == str\n            assert type(remaining_cells[0]) == str\n            if cell_id not in remaining_cells:\n                # print(f\"Cell {cell_id} not in remaining cells. Skipping.\")\n                continue\n            if len(cell_data[\"matrices\"]) != n_chromosomes:\n                # print(f\"Cell {cell_id} has {len(cell_data['matrices'])} matrices, expected {n_chromosomes}. Skipping.\")\n                continue\n\n            feature = self._compute_cell_embeddings(cell_id, cell_data[\"matrices\"])\n            if feature.size == 0:\n                logging.warning(f\"Empty feature for cell: {cell_id} in {cell_type}\")\n            features_batch.append(feature)\n            labels_batch.append((cell_type, cell_id))\n\n        logging.info(\n            f\"Processed {len(features_batch)} features for cell type {cell_type}\"\n        )\n        return features_batch, labels_batch\n\n    def _compute_cell_embeddings(self, cell_id: str, matrices: list):\n        \"\"\"here we transform each of the chromosome matrix from each cell\"\"\"\n        cell_embedding_parts = []\n\n        if self.config.gnn:  # using GNN\n            for matrix in matrices:\n                # apply log2 transformation as in the paper\n                matrix = np.log2(matrix + 1)\n                if self.config.enhance:\n                    if self.config.enhance.enhance_method == \"hicmfr\":\n                        hicmfr_config = {\n                            \"reinsertion_rate\": self.config.enhance.reinsertion_rate,\n                            \"n_components\": self.config.enhance.n_components,\n                            \"conv\": self.config.enhance.conv,\n                            \"mf\": self.config.enhance.mf\n                        }\n                        matrix = HiCMFR(matrix, **hicmfr_config)\n                    else:\n                        raise ValueError(\"Unknown enhancing method\")\n                # GNN's input dimension\n                num_bins = matrix.shape[0]\n                # Extract edges from contact matrix\n                edge_index = np.array(np.nonzero(matrix))\n                # Assign interaction weights\n                edge_attr = matrix[edge_index[0], edge_index[1]]\n                # Node features (interaction strengths)\n                x = torch.tensor(matrix, dtype=torch.float).to(self.config.gnn.device)\n                edge_index = torch.tensor(edge_index, dtype=torch.long).to(self.config.gnn.device)\n                edge_attr = torch.tensor(edge_attr, dtype=torch.float)\n\n                data = Data(x=x, edge_index=edge_index, edge_attr=edge_attr)\n\n                if self.config.gnn.model == \"GNN\":\n                    model = HiCGNN(input_dim=num_bins, hidden_dim=self.config.gnn.hidden_dim, output_dim=self.config.gnn.output_dim)\n                elif self.config.gnn.model == \"GAT\":\n                    model = HiCGAT(input_dim=num_bins, hidden_dim=self.config.gnn.hidden_dim, output_dim=self.config.gnn.output_dim, num_layers=5, conv_type=self.config.gnn.conv_type)\n                elif self.config.gnn.model == \"SAGE\":\n                    model = HiCGAT(input_dim=num_bins, hidden_dim=self.config.gnn.hidden_dim, output_dim=self.config.gnn.output_dim, num_layers=5, conv_type=self.config.gnn.conv_type)\n                else:\n                    raise ValueError(f\"Unknown model type: {self.config.gnn.model}\")\n\n                model.to(self.config.gnn.device)\n                chromosome_embeddings = model(data.x, data.edge_index).detach().cpu()\n                cell_embedding_parts.append(chromosome_embeddings.detach().numpy().flatten())\n\n        elif self.config.pca_n_components:  # using PCA\n            for matrix in matrices:\n                # apply log2 transformation as in the paper\n                matrix = np.log2(matrix + 1)\n                if self.config.enhance:\n                    if self.config.enhance.enhance_method == \"hicmfr\":\n                        hicmfr_config = {\n                            \"reinsertion_rate\": self.config.enhance.reinsertion_rate,\n                            \"n_components\": self.config.enhance.n_components,\n                            \"conv\": self.config.enhance.conv,\n                            \"mf\": self.config.enhance.mf\n                        }\n                        matrix = HiCMFR(matrix, **hicmfr_config)\n                    else:\n                        raise ValueError(\"Unknown enhancing method\")\n                # logging.info(f\"Cell {cell_id} matrix shape: {matrix.shape}\")\n                matrix = get_n_pc(cell_id, matrix, n_components=self.config.pca_n_components)\n                # logging.info(f\"Cell {cell_id} PCA shape: {matrix.shape}\")\n                cell_embedding_parts.append(matrix.flatten())\n\n        else:\n            raise ValueError(\"Either GNN or PCA must be specified in the config.\")\n\n        embeddings = np.concatenate(cell_embedding_parts)\n        return embeddings\n\n    def _truncate_features(self, features):\n        \"\"\"Truncate features to the minimum length found in the feature array.\"\"\"\n        min_length = min(feature.shape[0] for feature in features)\n        logging.info(f\"Minimum length of features: {min_length}\")\n        features = [feature[:min_length] for feature in features]\n        logging.info(\"Done truncating features to the minimum length.\")\n        return np.array(features)\n\n    def _reduce_features(self, features) -> np.ndarray:\n        logging.info(f\"feature shape before reduction: {features.shape}\")\n        pca = PCA(n_components=self.config.n_components)\n        # n_components = features.shape[0] // 2\n        # pca = PCA(n_components=n_components)\n        # logging.info(f\"Applying PCA for clustering with {n_components} components.\")\n        return pca.fit_transform(features)\n\n    def cluster(self):\n        if self.dataset is None:\n            raise ValueError(\"Dataset is not set. Call setup_dataset first.\")\n        if self.features_reduced is None:\n            raise ValueError(\"Features are not built. Call build_features first.\")\n        if self.cell_labels is None:\n            raise ValueError(\"Cell labels are not built. Call build_features first.\")\n        if self.cell_type_id_bins is None:\n            raise ValueError(\"Cell ID bins are not loaded. Call load_data first.\")\n\n        num_clusters = len(self.dataset.cell_types)\n        logging.info(f\"Clustering into: {num_clusters} clusters.\")\n        clustering_method = KMeans(n_clusters=num_clusters)\n        self.clusters = clustering_method.fit_predict(self.features_reduced)\n\n    def evaluate(self, base_path: Path):\n        if self.cell_labels is None:\n            raise ValueError(\"Cell labels are not built. Call build_features first.\")\n        true_labels = [cell_type for cell_type, _ in self.cell_labels]\n        ari = ari_score(true_labels, self.clusters)\n        # average_methoh: 'min', 'geometric', 'arithmetic', 'max'\n        nmi = nmi_score(true_labels, self.clusters, average_method=self.config.nmi_avg_method)\n        hm = hm_score(true_labels, self.clusters)\n        fm = fm_score(true_labels, self.clusters, is_sparse=True)\n\n        self.metrics = {\"ARI\": ari, \"NMI\": nmi, \"HM\": hm, \"FM\": fm}\n        plot_evaluation_scores(\n            base_path=base_path,\n            metrics=self.metrics,\n            file_name=f\"{self.timestamp}_evaluation_scores.png\",\n        )\n\n        for metric, value in self.metrics.items():\n            logging.info(f\"{metric}: {value}\")\n\n    def visualize(self, file_path: Path) -> None:\n        if self.features_reduced is None:\n            raise ValueError(\"Features are not built. Call build_features first.\")\n        if self.cell_labels is None:\n            raise ValueError(\"Cell labels are not built. Call build_features first.\")\n        # PCA visualization\n        plot_pca_clusters(\n            features_reduced=self.features_reduced,\n            clusters=self.cell_labels,\n            file_path=file_path,\n            file_name=f\"{self.timestamp}_pca.png\",\n        )\n\n        # UMAP visualization\n        if self.dataset is None:\n            raise ValueError(\"Dataset is not set. Please call setup_dataset first.\")\n\n        reducer = umap_reducer(self.dataset.name)\n        plot_umap_cell_types(\n            features=self.features_reduced,\n            cell_labels=self.cell_labels,\n            reducer=reducer,\n            file_path=file_path,\n            file_name=f\"{self.timestamp}_umap.png\",\n        )\n\n    def run(self, data_path: Path, name: str, species: str, base_path: Path):\n        \"\"\"pipeline starting point\"\"\"\n        self.setup_dataset(data_path, name, species)\n        self.load_data(base_path)\n        self.build_features()\n        self.cluster()\n        if self.dataset is None:\n            raise ValueError(\"Dataset is not set. Call setup_dataset first.\")\n        self.evaluate(base_path.parent / \"result\" / self.dataset.name)\n        self.visualize(file_path=base_path.parent / \"result\" / self.dataset.name)\n\n\ndef main():\n    pass\n\n\nif __name__ == \"__main__\":\n    main()\n```\n\nreview this code.\n# Code Review: `pipeline.py`\n\nThe code implements a pipeline for clustering single-cell Hi-C data. Below is my comprehensive review:\n\n## Strengths\n\n1. **Well-structured class hierarchy**: The `Pipeline` class is organized logically with clear steps from data loading to visualization.\n\n2. **Error handling**: The code includes appropriate validation checks at each step to ensure previous steps have been completed.\n\n3. **Flexibility**: The pipeline supports both GNN-based and PCA-based feature extraction methods, offering versatility.\n\n4. **Comprehensive logging**: Good use of logging throughout the code to track progress and debug issues.\n\n5. **Type hints**: Appropriate use of type annotations to improve code readability and maintainability.\n\n## Areas for Improvement\n\n1. **Error handling in feature extraction**: The `_compute_cell_embeddings` method doesn't handle potential exceptions from model creation or inference.\n\n2. **Memory management**: Large matrices are processed without explicit memory cleanup, which might cause issues with large datasets.\n\n3. **Hardcoded values**: \n   - The chromosome count logic is somewhat rigid (`n_chromosomes = 23 if self.dataset.species == \"human\" else 20`)\n   - Some parameters like `num_layers=5` in the GNN model creation are hardcoded rather than configurable\n\n4. **Code duplication**: There's repetition in the enhancing logic for both GNN and PCA paths in `_compute_cell_embeddings`.\n\n5. **Lack of model persistence**: The GNN models are recreated for each cell matrix rather than being reused, which is inefficient.\n\n6. **Limited error reporting**: When features are empty, the warning is logged but the processing continues, which might lead to failures later.\n\n## Suggestions\n\n1. **Extract common functionality**: Create helper methods for the data enhancement logic to avoid duplication.\n\n2. **Cache GNN models**: Create and reuse models for efficiency rather than recreating them for each matrix.\n\n3. **Make chromosome count configurable**: Move this to the config rather than hardcoding it.\n\n4. **Add more robust exception handling**: Especially in the feature extraction steps.\n\n5. **Consider chunking for large datasets**: Process cells in batches to manage memory more effectively.\n\n6. **Add parallelization**: The feature extraction step could benefit from parallel processing.\n\n7. **Add input validation**: Validate matrix shapes and other inputs before processing.\n\n8. **Improve visualization options**: Allow more customization of the visualization outputs.\n\nOverall, the code is well-structured and follows good practices, but could benefit from these optimizations for better performance, maintainability, and error handling.\n","messages":[{"id":{"replica_id":0,"value":0},"start":0,"metadata":{"role":"user","status":"Done","timestamp":{"replica_id":0,"value":28}}},{"id":{"replica_id":0,"value":18},"start":13768,"metadata":{"role":"assistant","status":"Done","timestamp":{"replica_id":0,"value":26}}},{"id":{"replica_id":0,"value":19},"start":16465,"metadata":{"role":"user","status":"Done","timestamp":{"replica_id":0,"value":27}}}],"summary":"ScHiC-GNN Pipeline Code Review","slash_command_output_sections":[{"range":{"start":0,"end":1},"icon":"Library","label":"Default","metadata":null},{"range":{"start":3,"end":13749},"icon":"File","label":"schic-gnn/schicgnn/pipeline.py","metadata":{"path":"schic-gnn/schicgnn/pipeline.py"}}]}